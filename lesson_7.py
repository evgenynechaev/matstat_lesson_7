import math
import numpy as np
import matplotlib.pyplot as plt

zp = [35, 45, 190, 200, 40, 70, 54, 150, 120, 110]
ks = [401, 574, 874, 919, 459, 739, 653, 902, 746, 832]


def sse_(x, y, a, b):
    return ((a + b * x - y) ** 2).sum()


def mse_(b1, x, y):
    n = len(x)
    return np.sum((b1 * x - y) ** 2) / n


def task_1():
    """
    Задача 1:
        Даны значения величины заработной платы заемщиков банка (zp)
        и значения их поведенческого кредитного скоринга (ks):
        zp = [35, 45, 190, 200, 40, 70, 54, 150, 120, 110],
        ks = [401, 574, 874, 919, 459, 739, 653, 902, 746, 832].
        Используя математические операции, посчитать коэффициенты линейной регрессии,
        приняв за X заработную плату (то есть, zp - признак),
        а за y - значения скорингового балла (то есть, ks - целевая переменная).
        Произвести расчет как с использованием intercept, так и без.
    Ответ:
        Расчет без использования intercept:
        b = 2.6205
        формула: y_hat = 2.6205 * x

        Расчет с использованием intercept
        a = 444.1774
        b = 2.6205
        формула: y_hat = 444.1774 + 2.6205 * x
    """
    print("Задача 1")
    x = np.array(zp)
    y = np.array(ks)
    print(f"    Расчет без intercept")
    b_no_intercept = (np.mean(x * y) - np.mean(x) * np.mean(y)) / (np.mean(x ** 2) - np.mean(x) ** 2)
    print(f"        b = {b_no_intercept:.4f}")
    plt.scatter(x, y)
    plt.plot(x, b_no_intercept * x)
    plt.show()

    print()
    print(f"    Расчет с использованием intercept")
    n = len(zp)
    b = (n * (np.sum(x * y)) - (np.sum(x) * np.sum(y))) / (n * (np.sum(x ** 2)) - (np.sum(x) ** 2))
    a = np.mean(y) - b * np.mean(x)
    print(f"        b = {b:.4f}")
    print(f"        a = {a:.4f}")
    print()
    plt.scatter(x, y)
    plt.plot(x, a + b * x)
    plt.show()


def task_2():
    """
    Задача 2:
        Посчитать коэффициент линейной регрессии при заработной плате (zp),
        используя градиентный спуск (без intercept).
    Решение:
        Расчет коэффициента линейной регрессии методом градиентного спуска.
          1. B1 = 9.8867514, mse = 276604.34287384455
        101. B1 = 6.134338965197076, mse = 57340.5519462072
        201. B1 = 5.904779227072703, mse = 56519.94114908718
        301. B1 = 5.890735548696904, mse = 56516.86995307483
        401. B1 = 5.889876404563293, mse = 56516.8584588988
        501. B1 = 5.889823845068368, mse = 56516.85841588101
        601. B1 = 5.88982062965852, mse = 56516.85841572002
        701. B1 = 5.889820432950761, mse = 56516.85841571941
        801. B1 = 5.8898204209168545, mse = 56516.85841571941
        829. B2 = 5.889820420491323, mse = 56516.85841571941
        830. B1 = 5.889820420481441, mse = 56516.85841571941
    Ответ:
        При использовании метода градиентного спуска коэффициент линейной регрессии составляет:
        b = 5.88982042
    """
    print("Задача 2")
    print(f"    Расчет коэффициента линейной регрессии методом градиентного спуска.")
    x = np.array(zp)
    y = np.array(ks)
    alpha = 1e-6
    b1 = 10
    b2 = 100
    n = len(zp)
    loops = 10000
    i = 0
    for i in range(loops):
        b1 -= alpha * (2 / n) * np.sum((b1 * x - y) * x)
        if i % 100 == 0:
            print(f"    {i + 1:4}. B1 = {b1}, mse = {mse_(b1, x, y)}")
        if abs(b1 - b2) < 1e-11:
            # прекратить расчет, если погрешность результата составляет менее 1e-11.
            break
        b2 = b1
    print(f"    {i:4}. B2 = {b2}, mse = {mse_(b1, x, y)}")
    print(f"    {i + 1:4}. B1 = {b1}, mse = {mse_(b1, x, y)}")
    print(f"Ответ:")
    print(f"    При использовании метода градиентного спуска коэффициент линейной регрессии составляет:")
    print(f"    b = {b1:.8f}")
    print()


def task_3():
    """
    Задача 3:
        В каких случаях для вычисления доверительных интервалов и проверки статистических гипотез
        используется таблица значений функции Лапласа,
        а в каких - таблица критических точек распределения Стьюдента?
    Ответ:
        Для вычисления доверительных интервалов:

        Таблица значений функции Лапласа - это вероятность того, что случайная величина примет значение,
        принадлежащее заданному интервалу.
        Таблица значений функции Лапласа используется в случае, когда известно среднее квадратическое
        отклонение генеральной совокупности. Табличное значение - это коэффициент доверия или вероятность того,
        что случайная величина примет значение, принадлежащее заданному интервалу.

        Таблица критических точек распределения Стьюдента используется, когда среднее квадратическое
        отклонение генеральной совокупности не известно.
         t-критерий Стьюдента – общее название для класса методов статистической проверки гипотез
        (статистических критериев), основанных на распределении Стьюдента.
    """
    ...


def task_4_star():
    """
    Задача *4:
        Произвести вычисления как в пункте 2, но с вычислением intercept.
        Учесть, что изменение коэффициентов должно производиться на каждом шаге одновременно
        (то есть изменение одного коэффициента не должно влиять на изменение другого во время одной итерации).
    Решение:
        Цель, расчитанная в задаче 1:
        a = 444.1774
        b = 2.6205
        Расчет коэффициента линейной регрессии с параметром intercept методом градиентного спуска.
             0.  a=0.070990, b=8.114170, sse=13.343070
        100000.  a=409.029730, b=2.879241, sse=10.431728
        200000.  a=441.395794, b=2.641012, sse=10.384737
        300000.  a=443.957226, b=2.622159, sse=10.384435
        400000.  a=444.159936, b=2.620667, sse=10.384433
        500000.  a=444.175979, b=2.620549, sse=10.384433
        559357.  a = 444.177051, b = 2.620541, sse = 10.384433    Ответ:
        На 559357 шаге полученные рассчитанные изменяются мало, останвливаем расчет.
    Ответ:
        При использовании метода градиентного спуска коэффициенты линейной регрессии составляют:
        a = 444.177051
        b = 2.620541
        формула: y = 444.18 + 2.62 * x
    """
    print("Задача 4")
    print(f"    Расчет коэффициента линейной регрессии с параметром intercept методом градиентного спуска.")
    # формула с двумя коэффициентами: y = a + b * x
    # Цель, расчитанная в задаче 1:
    # a = 444.1774
    # b = 2.6205

    x = np.array(zp)
    y = np.array(ks)
    alpha = 1e-5  # Эмпирически выводим инкремент. При больших значениях (1e-4) происходит расхождение
    epochs = 1000000  # Количество итераций
    a = 0  # Стартовое значение параметра a
    b = 0  # Стартовое значение параметра b

    a_next = a  # Инкрементное значение параметра a
    b_next = b  # Инкрементное значение параметра b
    a_prev = a_next  # Предыдущее значение параметра a
    b_prev = b_next  # Предыдущее значение параметра b
    # Логарифм от предыдущего значения суммы квадратичных ошибок всех элементов обучающей выборки:
    sse_prev = math.log(sse_(x, y, a_next, b_next) / 2)
    sse_next = sse_prev
    divergence_count = 0  # Счетчик для вычисления присутствия изменений параметров при расчете
    is_convergence = True  # Выполняется ли сходимость
    no_changes = 0  # Счетчик при отсутствии инкрементации переменных
    i = 0

    for i in range(epochs):
        gradient_a = (a_next + b_next * x - y).sum()  # градиент параметра a
        delta_a = -alpha * gradient_a  # изменение градиента параметра a

        gradient_b = ((a_next + b_next * x - y) * x).sum()  # градиент параметра b
        delta_b = -alpha * gradient_b  # изменение градиента параметра b

        a_next += delta_a  # следующая итерация параметра a
        b_next += delta_b  # следующая итерация параметра b

        # для расчета используем логарифм от суммы квадратичных ошибок
        sse_next = math.log(sse_(x, y, a_next, b_next) / 2)

        # на каждую 100000-ую операцию выводим сообщение:
        print(f"    {i:6}.  a={a_next:.6f}, b={b_next:.6f}, sse={sse_next:.6f}") if i % 100000 == 0 else ...

        # если началось расхождение
        if sse_prev < sse_next:
            divergence_count += 1
            if divergence_count > 10:  # если на протяжении 10 циклов происходит расхождение, завершаем расчет с ошибкой
                is_convergence = False  # нет сходимости
                break
        else:
            divergence_count = 0  # сброс счетчика

        # если нет изменений в расчете параметров
        if abs(sse_prev - sse_next) < 1e-6 and abs(a_prev - a_next) < 1e-8 and abs(b_prev - b_next) < 1e-8:
            no_changes += 1
            if no_changes > 10000:  # если на протяжении 10000 циклов нет изменений, завершаем расчет
                break
        else:
            no_changes = 0  # сброс счетчика

        # следующее значение становится предыдущим
        sse_prev = sse_next
        a_prev = a_next
        b_prev = b_next

    # если расчет сходился:
    if is_convergence:
        print(f"    {i:6}.  a = {a_next:.6f}, b = {b_next:.6f}, sse = {sse_next:.6f}")
    else:
        # если обнаружено расхождение
        print(f"    Происходит расхождение вычислений к бесконечности.")
        print(f"    Необходимо уменьшить инкремент alpha для выполнения сходимости.")
        return

    a = a_next
    b = b_next
    print(f"Ответ:")
    print(f"    При использовании метода градиентного спуска коэффициенты линейной регрессии составляют:")
    print(f"    a = {a:.6f}")
    print(f"    b = {b:.6f}")
    print(f"    формула: y = {a:.2f} + {b:.2f} * x")
    print()
    print(f"    Проверка при помощи метода расчета по сетке значений в диапазоне:")

    test_a_min = 200.0
    test_a_max = 600.0
    test_a_samples = 100  # количество проб
    print(f"    a = [{test_a_min} ; {test_a_max}]")

    test_b_min = 1.0
    test_b_max = 4.0
    test_b_samples = 100  # количество проб
    print(f"    b = [{test_b_min} ; {test_b_max}]")

    test_a_array = np.linspace(test_a_min, test_a_max, test_a_samples)
    test_b_array = np.linspace(test_b_min, test_b_max, test_b_samples)
    test_a_matrix, test_b_matrix = np.meshgrid(test_a_array, test_b_array)

    test_sse = []
    for j in range(len(test_b_array)):
        test_sse.append([])
        for i in range(len(test_a_array)):
            test_sse[j].append(((test_a_matrix[j][i] + test_b_matrix[j][i] * x - y) ** 2).sum())
    test_sse = np.array(test_sse)
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')
    ax.set_xlabel('a')
    ax.set_ylabel('b')
    ax.set_zlabel('log(SSE/2)')
    ax.plot_wireframe(test_a_matrix, test_b_matrix, np.log(test_sse / 2),
                      color='lightblue', rstride=8, cstride=8, label='log(SSE/2)')

    # Поиск минимального значения на сетке
    # координаты минимальной точки сетки:
    min_ind = np.unravel_index(np.argmin(test_sse), test_sse.shape)
    ax.scatter(test_a_matrix[min_ind], test_b_matrix[min_ind], math.log(test_sse[min_ind] / 2),
               color='red', marker='o', s=100,
               label=f'min: a={test_a_matrix[min_ind]:.2f}, b={test_b_matrix[min_ind]:.2f}')
    plt.xlim(test_a_min, test_a_max)
    plt.ylim(test_b_min, test_b_max)

    plt.legend()
    plt.show()
    print(f"    Минимальное расчетное значение: a = {test_a_matrix[min_ind]:.2f}, b = {test_b_matrix[min_ind]:.2f}.")
    print()
    print(f"    Вывод: рассчитанные методом градиентного спуска значения параметров близки к значениям, "
          f"рассчитанным теоретически в задаче 1.")
    print(f"    a = {a:.6f}")
    print(f"    b = {b:.6f}")
    print(f"    формула: y = {a:.2f} + {b:.2f} * x")
    print()


task_1()
task_2()
task_3()
task_4_star()
